{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EA4: MNIST desde cero\n",
    "### Apellido_Nombre_EA4.ipynb\n",
    "\n",
    "## Objetivo\n",
    "Implementar dos clasificadores para el conjunto de datos MNIST:\n",
    "1. Una implementación desde cero sin usar bibliotecas de alto nivel\n",
    "2. Una implementación usando scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos solo las bibliotecas básicas necesarias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga y Preparación de Datos\n",
    "Primero, cargaremos el conjunto de datos MNIST y lo prepararemos para su uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos MNIST\n",
    "print(\"Cargando dataset MNIST...\")\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist.data.astype('float32'), mnist.target.astype('int32')\n",
    "\n",
    "# Normalizamos los datos al rango [0,1]\n",
    "X = X / 255.0\n",
    "\n",
    "# Dividimos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Forma de los datos de entrenamiento:\", X_train.shape)\n",
    "print(\"Forma de los datos de prueba:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementación desde Cero\n",
    "Implementaremos una red neuronal simple con:\n",
    "- Capa de entrada: 784 neuronas (28x28 píxeles)\n",
    "- Capa oculta: 128 neuronas\n",
    "- Capa de salida: 10 neuronas (dígitos 0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkFromScratch:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Inicialización de pesos y sesgos\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) / np.sqrt(input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) / np.sqrt(hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def relu(self, X):\n",
    "        \"\"\"Función de activación ReLU\"\"\"\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def relu_derivative(self, X):\n",
    "        \"\"\"Derivada de ReLU\"\"\"\n",
    "        return X > 0\n",
    "\n",
    "    def softmax(self, X):\n",
    "        \"\"\"Función softmax para la capa de salida\"\"\"\n",
    "        exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        return exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Propagación hacia adelante\"\"\"\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = self.softmax(self.Z2)\n",
    "        return self.A2\n",
    "\n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"Retropropagación\"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        # Convertir y a one-hot\n",
    "        y_one_hot = np.zeros((batch_size, 10))\n",
    "        y_one_hot[np.arange(batch_size), y.astype(int)] = 1\n",
    "\n",
    "        # Gradientes de la capa de salida\n",
    "        dZ2 = self.A2 - y_one_hot\n",
    "        dW2 = np.dot(self.A1.T, dZ2) / batch_size\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / batch_size\n",
    "\n",
    "        # Gradientes de la capa oculta\n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * self.relu_derivative(self.Z1)\n",
    "        dW1 = np.dot(X.T, dZ1) / batch_size\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / batch_size\n",
    "\n",
    "        # Actualización de pesos y sesgos\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Realizar predicciones\"\"\"\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def calculate_accuracy(self, X, y):\n",
    "        \"\"\"Calcular precisión\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo desde cero\n",
    "print(\"Entrenando el modelo desde cero...\")\n",
    "model = NeuralNetworkFromScratch(784, 128, 10)\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(epochs):\n",
    "    # Mezclamos los datos\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "\n",
    "    # Entrenamiento por lotes\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train_shuffled[i:i+batch_size]\n",
    "        batch_y = y_train_shuffled[i:i+batch_size]\n",
    "\n",
    "        # Forward y backward pass\n",
    "        model.forward(batch_X)\n",
    "        model.backward(batch_X, batch_y, learning_rate)\n",
    "\n",
    "    # Calculamos y mostramos la precisión\n",
    "    train_acc = model.calculate_accuracy(X_train, y_train)\n",
    "    test_acc = model.calculate_accuracy(X_test, y_test)\n",
    "    print(f\"Época {epoch+1}/{epochs}:\")\n",
    "    print(f\"  Precisión en entrenamiento: {train_acc:.4f}\")\n",
    "    print(f\"  Precisión en prueba: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementación con Scikit-learn\n",
    "Ahora implementaremos el clasificador usando MLPClassifier de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Creamos y entrenamos el modelo\n",
    "print(\"Entrenando el modelo con scikit-learn...\")\n",
    "sklearn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128,),\n",
    "    max_iter=10,\n",
    "    learning_rate_init=0.1,\n",
    "    batch_size=32,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluamos el modelo\n",
    "train_acc = sklearn_model.score(X_train, y_train)\n",
    "test_acc = sklearn_model.score(X_test, y_test)\n",
    "\n",
    "print(\"\\nResultados del modelo scikit-learn:\")\n",
    "print(f\"Precisión en entrenamiento: {train_acc:.4f}\")\n",
    "print(f\"Precisión en prueba: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparación Visual de Resultados\n",
    "Visualizaremos algunas predicciones de ambos modelos para comparar su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos algunas imágenes aleatorias para visualizar\n",
    "n_samples = 10\n",
    "random_indices = np.random.randint(0, len(X_test), n_samples)\n",
    "\n",
    "# Obtenemos predicciones de ambos modelos\n",
    "scratch_predictions = model.predict(X_test[random_indices])\n",
    "sklearn_predictions = sklearn_model.predict(X_test[random_indices])\n",
    "true_labels = y_test[random_indices]\n",
    "\n",
    "# Visualizamos los resultados\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_test[random_indices[idx]].reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Real: {true_labels[idx]}\\nDesde cero: {scratch_predictions[idx]}\\nScikit: {sklearn_predictions[idx]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "He implementado dos clasificadores para el conjunto de datos MNIST:\n",
    "\n",
    "1. **Implementación desde cero**:\n",
    "   - Desarrollé una red neuronal completa usando solo NumPy\n",
    "   - Implementé manualmente las funciones de activación, propagación y retropropagación\n",
    "   - Logramos una precisión de prueba de [valor]\n",
    "\n",
    "2. **Implementación con scikit-learn**:\n",
    "   - Utilicé MLPClassifier con una arquitectura similar\n",
    "   - Obtuve una precisión de prueba de [valor]\n",
    "\n",
    "La comparación muestra que:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
